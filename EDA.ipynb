{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from bamf.bamfCR import *\n",
    "\n",
    "import time\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# set plot parameters\n",
    "params = {'legend.fontsize': 18,\n",
    "          'figure.figsize': (8, 7),\n",
    "          'axes.labelsize': 24,\n",
    "          'axes.titlesize':24,\n",
    "          'axes.linewidth':3,\n",
    "          'xtick.labelsize':20,\n",
    "          'ytick.labelsize':20}\n",
    "plt.rcParams.update(params)\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treatments</th>\n",
       "      <th>Time</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>s9</th>\n",
       "      <th>s10</th>\n",
       "      <th>s11</th>\n",
       "      <th>s12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exp_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020140</td>\n",
       "      <td>0.081164</td>\n",
       "      <td>0.046799</td>\n",
       "      <td>0.080794</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.055159</td>\n",
       "      <td>0.093193</td>\n",
       "      <td>0.058218</td>\n",
       "      <td>0.020610</td>\n",
       "      <td>0.071776</td>\n",
       "      <td>0.037899</td>\n",
       "      <td>0.066838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exp_1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.158808</td>\n",
       "      <td>0.037643</td>\n",
       "      <td>0.107039</td>\n",
       "      <td>0.262354</td>\n",
       "      <td>0.021680</td>\n",
       "      <td>0.112563</td>\n",
       "      <td>0.082086</td>\n",
       "      <td>0.083350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014638</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.123679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exp_1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.165128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161716</td>\n",
       "      <td>0.290950</td>\n",
       "      <td>0.068935</td>\n",
       "      <td>0.118747</td>\n",
       "      <td>0.094958</td>\n",
       "      <td>0.093735</td>\n",
       "      <td>0.024552</td>\n",
       "      <td>0.013541</td>\n",
       "      <td>0.028713</td>\n",
       "      <td>0.129184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exp_10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042981</td>\n",
       "      <td>0.087288</td>\n",
       "      <td>0.035596</td>\n",
       "      <td>0.092976</td>\n",
       "      <td>0.014878</td>\n",
       "      <td>0.094003</td>\n",
       "      <td>0.083272</td>\n",
       "      <td>0.084605</td>\n",
       "      <td>0.012392</td>\n",
       "      <td>0.059649</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>0.072118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exp_10</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.180660</td>\n",
       "      <td>0.021611</td>\n",
       "      <td>0.093696</td>\n",
       "      <td>0.285043</td>\n",
       "      <td>0.095564</td>\n",
       "      <td>0.130708</td>\n",
       "      <td>0.113519</td>\n",
       "      <td>0.127640</td>\n",
       "      <td>0.011734</td>\n",
       "      <td>0.024132</td>\n",
       "      <td>0.010197</td>\n",
       "      <td>0.140246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>exp_8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.081683</td>\n",
       "      <td>0.033165</td>\n",
       "      <td>0.105040</td>\n",
       "      <td>0.319787</td>\n",
       "      <td>0.105994</td>\n",
       "      <td>0.082605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108795</td>\n",
       "      <td>0.064975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036798</td>\n",
       "      <td>0.111022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>exp_8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.111730</td>\n",
       "      <td>0.015665</td>\n",
       "      <td>0.128014</td>\n",
       "      <td>0.331796</td>\n",
       "      <td>0.135531</td>\n",
       "      <td>0.058358</td>\n",
       "      <td>0.010763</td>\n",
       "      <td>0.088152</td>\n",
       "      <td>0.094879</td>\n",
       "      <td>0.013920</td>\n",
       "      <td>0.013347</td>\n",
       "      <td>0.161240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>exp_9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063205</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.088759</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.077716</td>\n",
       "      <td>0.004590</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>0.097105</td>\n",
       "      <td>0.087168</td>\n",
       "      <td>0.071016</td>\n",
       "      <td>0.095851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>exp_9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.174650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158850</td>\n",
       "      <td>0.097460</td>\n",
       "      <td>0.069699</td>\n",
       "      <td>0.115715</td>\n",
       "      <td>0.046565</td>\n",
       "      <td>0.061763</td>\n",
       "      <td>0.040231</td>\n",
       "      <td>0.027781</td>\n",
       "      <td>0.037776</td>\n",
       "      <td>0.195518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>exp_9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.132391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>0.278131</td>\n",
       "      <td>0.159930</td>\n",
       "      <td>0.092295</td>\n",
       "      <td>0.114139</td>\n",
       "      <td>0.078319</td>\n",
       "      <td>0.046285</td>\n",
       "      <td>0.023038</td>\n",
       "      <td>0.044428</td>\n",
       "      <td>0.150090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Treatments  Time        s1        s2        s3        s4        s5  \\\n",
       "0        exp_1   0.0  0.020140  0.081164  0.046799  0.080794  0.000743   \n",
       "1        exp_1   8.0  0.158808  0.037643  0.107039  0.262354  0.021680   \n",
       "2        exp_1  16.0  0.165128  0.000000  0.161716  0.290950  0.068935   \n",
       "3       exp_10   0.0  0.042981  0.087288  0.035596  0.092976  0.014878   \n",
       "4       exp_10   8.0  0.180660  0.021611  0.093696  0.285043  0.095564   \n",
       "..         ...   ...       ...       ...       ...       ...       ...   \n",
       "187      exp_8   8.0  0.081683  0.033165  0.105040  0.319787  0.105994   \n",
       "188      exp_8  16.0  0.111730  0.015665  0.128014  0.331796  0.135531   \n",
       "189      exp_9   0.0  0.063205  0.002620  0.088759  0.001612  0.012696   \n",
       "190      exp_9   8.0  0.174650  0.000000  0.158850  0.097460  0.069699   \n",
       "191      exp_9  16.0  0.132391  0.000000  0.159500  0.278131  0.159930   \n",
       "\n",
       "           s6        s7        s8        s9       s10       s11       s12  \n",
       "0    0.055159  0.093193  0.058218  0.020610  0.071776  0.037899  0.066838  \n",
       "1    0.112563  0.082086  0.083350  0.000000  0.014638  0.005025  0.123679  \n",
       "2    0.118747  0.094958  0.093735  0.024552  0.013541  0.028713  0.129184  \n",
       "3    0.094003  0.083272  0.084605  0.012392  0.059649  0.001639  0.072118  \n",
       "4    0.130708  0.113519  0.127640  0.011734  0.024132  0.010197  0.140246  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "187  0.082605  0.000000  0.108795  0.064975  0.000000  0.036798  0.111022  \n",
       "188  0.058358  0.010763  0.088152  0.094879  0.013920  0.013347  0.161240  \n",
       "189  0.077716  0.004590  0.071100  0.097105  0.087168  0.071016  0.095851  \n",
       "190  0.115715  0.046565  0.061763  0.040231  0.027781  0.037776  0.195518  \n",
       "191  0.092295  0.114139  0.078319  0.046285  0.023038  0.044428  0.150090  \n",
       "\n",
       "[192 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# used later for model validation\n",
    "# gLV_data = pd.read_csv(\"gLV_data/DSM_processed_mono.csv\")\n",
    "gLV_data = pd.read_csv(\"gLV_data/gLV_data_for_CR.csv\")\n",
    "gLV_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['s1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11',\n",
       "       's12'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get species names\n",
    "species = gLV_data.columns.values[2:]\n",
    "species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to make predictions on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimensions \n",
    "n_s = len(species)\n",
    "n_r = 3\n",
    "# input to NN includes species, resources (and maybe also time) \n",
    "n_x = n_s + n_r\n",
    "\n",
    "# CR parameters \n",
    "d = -3.*np.ones(n_s)\n",
    "C = np.random.uniform(-1., 0., [n_r, n_s])\n",
    "P = np.random.uniform(-5., -1., [n_r, n_s])\n",
    "K = np.ones(n_r)\n",
    "\n",
    "# dimension of hidden layer\n",
    "n_h = 4\n",
    "\n",
    "# map to hidden dimension\n",
    "p_std = 1./np.sqrt(n_x)\n",
    "W1 = p_std*np.random.randn(n_h, n_x)\n",
    "b1 = np.random.randn(n_h)\n",
    "\n",
    "# parameters to compute efficiency matrix\n",
    "p_std = 1./np.sqrt(n_h)\n",
    "W2 = p_std*np.random.randn(n_r+2*n_s, n_h) \n",
    "b2 = np.random.randn(n_r+2*n_s)\n",
    "\n",
    "# concatenate parameter initial guess\n",
    "params = np.concatenate((d, W1.flatten(), b1, C.flatten(), W2.flatten(), b2.flatten(), P.flatten(), K))\n",
    "\n",
    "# set prior so that C is sparse \n",
    "W1prior = np.zeros_like(W1)\n",
    "b1prior = np.zeros_like(b1)\n",
    "Cprior = -5.*np.ones([n_r, n_s]) \n",
    "Pprior = -5.*np.ones([n_r, n_s])\n",
    "W2prior = np.zeros_like(W2)\n",
    "b2prior = np.zeros_like(b2)\n",
    "\n",
    "# concatenate prior \n",
    "prior = np.concatenate((d, W1prior.flatten(), b1prior, Cprior.flatten(), W2prior.flatten(), b2prior.flatten(), Pprior.flatten(), K))\n",
    "\n",
    "n_params = len(params)\n",
    "n_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using consumer resource model  \n",
    "def system(t, x, params): \n",
    "    \n",
    "    # species \n",
    "    s = x[:n_s]\n",
    "    \n",
    "    # resources\n",
    "    r = jnp.exp(x[n_s:])\n",
    "    \n",
    "    # compute state \n",
    "    state = jnp.concatenate((s, r))\n",
    "    \n",
    "    # death rate\n",
    "    d = jnp.exp(params[:n_s])\n",
    "    \n",
    "    # map to hidden layer\n",
    "    W1 = np.reshape(params[n_s:n_s+n_x*n_h], [n_h, n_x])\n",
    "    b1 = params[n_s+n_x*n_h:n_s+n_x*n_h+n_h]\n",
    "    h1 = jnp.tanh(W1@state + b1)\n",
    "    \n",
    "    # maximum consumption rate parameters\n",
    "    Cmax = jnp.exp(np.reshape(params[n_s+n_x*n_h+n_h:n_s+n_x*n_h+n_h+n_r*n_s], [n_r, n_s]))\n",
    "    \n",
    "    # attractiveness of resource i to species j / consumption efficiency\n",
    "    W2 = np.reshape(params[n_s+n_x*n_h+n_h+n_r*n_s:n_s+n_x*n_h+n_h+n_r*n_s+(n_r+2*n_s)*n_h], [n_r+2*n_s, n_h])\n",
    "    b2 = np.reshape(params[n_s+n_x*n_h+n_h+n_r*n_s+(n_r+2*n_s)*n_h:n_s+n_x*n_h+n_h+n_r*n_s+(n_r+2*n_s)*n_h+n_r+2*n_s], [n_r+2*n_s])\n",
    "    h2 = jax.nn.sigmoid(W2@h1 + b2)\n",
    "    \n",
    "    # divide hidden layer into resource availability, species growth efficiency, resource production efficiency\n",
    "    f = h2[:n_r]\n",
    "    g = h2[n_r:n_r+n_s]\n",
    "    h = h2[n_r+n_s:]\n",
    "    \n",
    "    # update Consumption matrix according to resource attractiveness \n",
    "    C = jnp.einsum(\"i,ij->ij\", f, Cmax)\n",
    "    \n",
    "    # max production rate\n",
    "    Pmax = jnp.exp(jnp.reshape(params[n_s+n_x*n_h+n_h+n_r*n_s+(n_r+2*n_s)*n_h+n_r+2*n_s:n_s+n_x*n_h+n_h+n_r*n_s+(n_r+2*n_s)*n_h+n_r+2*n_s+n_r*n_s], [n_r, n_s]))\n",
    "    K = jnp.exp(params[n_s+n_x*n_h+n_h+n_r*n_s+(n_r+2*n_s)*n_h+n_r+2*n_s+n_r*n_s:])\n",
    "    \n",
    "    # scaled production rate\n",
    "    P = jnp.einsum(\"ij,j->ij\", Pmax, h)\n",
    "    \n",
    "    # rate of change of species \n",
    "    dsdt = s*(g*(C.T@r) - d)\n",
    "\n",
    "    # rate of change of log of resources \n",
    "    dlrdt = (1. - r/K) * ((P-C)@s) \n",
    "\n",
    "    return jnp.append(dsdt, dlrdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define observation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define observation matrices \n",
    "O = np.zeros([n_s, n_s+n_r])\n",
    "O[:n_s,:n_s] = np.eye(n_s)\n",
    "O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model to mono culture data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.88281056 -0.42854083 -2.92016665]\n",
      "Total samples: 128, Updated regularization: 1.00e-05\n",
      "Total weighted fitting error: 2.854\n",
      "Total weighted fitting error: 2.415\n",
      "Total weighted fitting error: 2.137\n",
      "Total weighted fitting error: 1.868\n",
      "Total weighted fitting error: 1.796\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 1.795741331635591\n",
      "       x: [-1.779e+00  6.008e-02 ...  8.453e-01  9.997e-01]\n",
      "     nit: 5\n",
      "     jac: [-8.832e-02 -1.574e+00 ...  3.313e-01  2.203e-04]\n",
      "    nfev: 11\n",
      "    njev: 11\n",
      "    nhev: 5\n",
      "Evidence -279.596\n",
      "Updating precision...\n",
      "Total samples: 128, Updated regularization: 2.04e-05\n",
      "Total weighted fitting error: 18.971\n",
      "Total weighted fitting error: 18.091\n",
      "Total weighted fitting error: 18.038\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 18.037609100341797\n",
      "       x: [-1.908e+00  1.340e-02 ...  8.296e-01  9.996e-01]\n",
      "     nit: 3\n",
      "     jac: [ 1.650e-01 -2.337e+00 ...  5.235e-01  3.217e-04]\n",
      "    nfev: 4\n",
      "    njev: 4\n",
      "    nhev: 3\n",
      "Evidence 1921.545\n",
      "Updating precision...\n",
      "Total samples: 128, Updated regularization: 3.20e-05\n",
      "Total weighted fitting error: 159.430\n",
      "Total weighted fitting error: 156.215\n",
      "Total weighted fitting error: 155.446\n",
      "Total weighted fitting error: 151.500\n",
      "Total weighted fitting error: 150.819\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 150.8194122314453\n",
      "       x: [-1.528e+00 -7.002e-02 ...  9.623e-01  9.981e-01]\n",
      "     nit: 5\n",
      "     jac: [-7.146e-02 -1.821e+01 ...  2.703e+00  3.942e-03]\n",
      "    nfev: 9\n",
      "    njev: 9\n",
      "    nhev: 5\n",
      "Evidence 3699.078\n",
      "Updating precision...\n",
      "Total samples: 128, Updated regularization: 5.65e-05\n",
      "Total weighted fitting error: 491.640\n",
      "Total weighted fitting error: 487.122\n",
      "Total weighted fitting error: 485.859\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 485.8592224121094\n",
      "       x: [-1.512e+00 -1.342e-01 ...  9.869e-01  9.982e-01]\n",
      "     nit: 3\n",
      "     jac: [-3.101e-01 -2.338e+01 ...  2.537e+00 -5.990e-03]\n",
      "    nfev: 5\n",
      "    njev: 5\n",
      "    nhev: 3\n",
      "Evidence 4505.480\n",
      "Updating precision...\n",
      "Total samples: 128, Updated regularization: 9.69e-05\n",
      "Total weighted fitting error: 643.533\n",
      "Total weighted fitting error: 636.365\n",
      "Total weighted fitting error: 635.485\n",
      "Total weighted fitting error: 627.225\n",
      "Total weighted fitting error: 619.122\n",
      "Total weighted fitting error: 613.401\n",
      "Total weighted fitting error: 601.920\n",
      "Total weighted fitting error: 589.333\n",
      "Total weighted fitting error: 585.107\n",
      "Total weighted fitting error: 576.847\n",
      "Total weighted fitting error: 574.397\n",
      "Total weighted fitting error: 563.701\n",
      "Total weighted fitting error: 551.068\n",
      "Total weighted fitting error: 549.688\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 549.6876831054688\n",
      "       x: [-1.267e-01  8.061e-03 ...  1.359e+00  9.976e-01]\n",
      "     nit: 14\n",
      "     jac: [-1.021e+01  2.043e+01 ... -1.005e+00  6.791e-02]\n",
      "    nfev: 20\n",
      "    njev: 20\n",
      "    nhev: 14\n",
      "Evidence 4725.745\n",
      "Updating precision...\n",
      "Total samples: 128, Updated regularization: 1.62e-04\n",
      "Total weighted fitting error: 647.730\n",
      "Total weighted fitting error: 635.403\n",
      "Total weighted fitting error: 633.748\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 633.7484130859375\n",
      "       x: [ 6.651e-02  1.543e-01 ...  1.421e+00  9.966e-01]\n",
      "     nit: 3\n",
      "     jac: [ 2.008e+01 -1.167e+02 ...  6.994e+00 -3.141e-02]\n",
      "    nfev: 4\n",
      "    njev: 4\n",
      "    nhev: 3\n",
      "Evidence 4961.807\n",
      "Updating precision...\n",
      "Total samples: 128, Updated regularization: 2.37e-04\n",
      "Total weighted fitting error: 706.686\n",
      "Total weighted fitting error: 696.922\n",
      "Total weighted fitting error: 696.291\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 696.2914428710938\n",
      "       x: [ 1.324e-01  1.877e-01 ...  1.434e+00  9.961e-01]\n",
      "     nit: 3\n",
      "     jac: [-1.714e+01 -1.888e+01 ... -4.165e-01  1.125e-02]\n",
      "    nfev: 4\n",
      "    njev: 4\n",
      "    nhev: 3\n",
      "Evidence 5057.238\n",
      "Updating precision...\n",
      "Total samples: 128, Updated regularization: 3.23e-04\n",
      "Total weighted fitting error: 737.830\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 737.82958984375\n",
      "       x: [ 1.432e-01  1.955e-01 ...  1.434e+00  9.961e-01]\n",
      "     nit: 1\n",
      "     jac: [-1.494e+01 -8.756e+00 ...  2.684e+00  3.107e-02]\n",
      "    nfev: 2\n",
      "    njev: 2\n",
      "    nhev: 1\n",
      "Evidence 5085.960\n",
      "Updating precision...\n",
      "Total samples: 128, Updated regularization: 4.21e-04\n",
      "Total weighted fitting error: 740.166\n",
      "Total weighted fitting error: 738.957\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 738.9567260742188\n",
      "       x: [ 2.322e-01  1.820e-01 ...  1.434e+00  9.954e-01]\n",
      "     nit: 2\n",
      "     jac: [-4.015e+01  1.890e+01 ... -7.289e+00 -4.232e-02]\n",
      "    nfev: 3\n",
      "    njev: 3\n",
      "    nhev: 2\n",
      "Evidence 5109.313\n",
      "Updating precision...\n",
      "Total samples: 128, Updated regularization: 5.22e-04\n",
      "Total weighted fitting error: 752.308\n",
      "Total weighted fitting error: 748.937\n",
      "Total weighted fitting error: 743.170\n",
      "Total weighted fitting error: 742.921\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 742.9212646484375\n",
      "       x: [ 2.920e-01  2.245e-01 ...  1.440e+00  9.948e-01]\n",
      "     nit: 4\n",
      "     jac: [-9.117e+00 -1.257e+01 ... -2.881e-01 -2.855e-03]\n",
      "    nfev: 5\n",
      "    njev: 5\n",
      "    nhev: 4\n",
      "Evidence 5132.419\n",
      "Updating precision...\n",
      "Total samples: 128, Updated regularization: 6.26e-04\n",
      "Total weighted fitting error: 756.984\n",
      "Total weighted fitting error: 754.777\n",
      "Total weighted fitting error: 750.751\n",
      "Total weighted fitting error: 743.893\n",
      "Total weighted fitting error: 743.832\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 743.83203125\n",
      "       x: [ 3.515e-01  3.005e-01 ...  1.487e+00  9.935e-01]\n",
      "     nit: 5\n",
      "     jac: [ 1.191e+01 -3.578e+01 ...  4.406e+00  1.279e-02]\n",
      "    nfev: 6\n",
      "    njev: 6\n",
      "    nhev: 5\n",
      "Evidence 5150.415\n",
      "Updating precision...\n",
      "Total samples: 128, Updated regularization: 7.32e-04\n",
      "Total weighted fitting error: 758.677\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 758.6773071289062\n",
      "       x: [ 3.571e-01  3.001e-01 ...  1.487e+00  9.935e-01]\n",
      "     nit: 1\n",
      "     jac: [-3.579e+01 -8.670e+00 ...  1.444e+00  1.940e-02]\n",
      "    nfev: 2\n",
      "    njev: 2\n",
      "    nhev: 1\n",
      "Evidence 5156.702\n",
      "Updating precision...\n",
      "Total samples: 128, Updated regularization: 8.40e-04\n",
      "Total weighted fitting error: 760.301\n",
      "Total weighted fitting error: 756.098\n",
      "Total weighted fitting error: 756.063\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 756.0629272460938\n",
      "       x: [ 3.777e-01  3.395e-01 ...  1.486e+00  9.930e-01]\n",
      "     nit: 3\n",
      "     jac: [-1.421e+01 -1.512e+00 ... -1.835e+00  2.511e-02]\n",
      "    nfev: 4\n",
      "    njev: 4\n",
      "    nhev: 3\n",
      "Evidence 5162.767\n",
      "Updating precision...\n",
      "Total samples: 128, Updated regularization: 9.53e-04\n",
      "Total weighted fitting error: 762.338\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 762.337890625\n",
      "       x: [ 3.803e-01  3.382e-01 ...  1.486e+00  9.930e-01]\n",
      "     nit: 1\n",
      "     jac: [-3.222e+01 -3.841e-01 ...  5.892e-01  1.488e-02]\n",
      "    nfev: 2\n",
      "    njev: 2\n",
      "    nhev: 1\n",
      "Evidence 5167.222\n",
      "Pass count  1\n",
      "Elapsed time 134.63s\n"
     ]
    }
   ],
   "source": [
    "r0 = np.random.uniform(-3, 0, n_r)\n",
    "print(r0)\n",
    "\n",
    "model = ODE(system = system, \n",
    "            dataframe=gLV_data,\n",
    "            C=O,\n",
    "            CRparams = params, \n",
    "            r0 = r0,\n",
    "            prior = prior,\n",
    "            species = species,\n",
    "            alpha_0=1e-5,\n",
    "            verbose=True)\n",
    "\n",
    "# fit to data \n",
    "t0 = time.time()\n",
    "model.fit(evidence_tol=1e-3, nlp_tol=1e-3, patience=1, max_fails=1)\n",
    "print(\"Elapsed time {:.2f}s\".format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(self):\n",
    "    # loop over each sample in dataset\n",
    "    for n_t, (t_eval, Y_batch) in self.dataset.items():\n",
    "\n",
    "        # split samples into batches\n",
    "        n_samples = Y_batch.shape[0]\n",
    "        for batch_inds in np.array_split(np.arange(n_samples), n_samples//self.batch_size):\n",
    "            \n",
    "            # run model using current parameters, output = [n_time, self.n_sys_vars]\n",
    "            outputs = np.nan_to_num(self.batchODEZ(t_eval, Y_batch[batch_inds], self.params[:self.n_r], self.params[self.n_r:]))\n",
    "            \n",
    "            \n",
    "def forward(self):\n",
    "    # loop over each sample in dataset\n",
    "    for n_t, (t_eval, Y_batch) in self.dataset.items():\n",
    "\n",
    "        # split samples into batches\n",
    "        for Y_measured in Y_batch:\n",
    "\n",
    "            # run model using current parameters, output = [n_time, self.n_sys_vars]\n",
    "            output = np.nan_to_num(self.runODEZ(t_eval, Y_measured, self.params[:self.n_r], self.params[self.n_r:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.batch_size = 4\n",
    "model.batch_size\n",
    "\n",
    "model.batchODEZ = jit(vmap(model.runODEZ, (None, 0, None, None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25 s Â± 28.8 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit batch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 s Â± 30.3 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit forward(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
