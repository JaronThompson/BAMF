{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from bamf.bamfCR import *\n",
    "\n",
    "import time\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# set plot parameters\n",
    "params = {'legend.fontsize': 18,\n",
    "          'figure.figsize': (8, 7),\n",
    "          'axes.labelsize': 24,\n",
    "          'axes.titlesize':24,\n",
    "          'axes.linewidth':3,\n",
    "          'xtick.labelsize':20,\n",
    "          'ytick.labelsize':20}\n",
    "plt.rcParams.update(params)\n",
    "plt.style.use('seaborn-colorblind')\n",
    "plt.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treatments</th>\n",
       "      <th>Time</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>s9</th>\n",
       "      <th>s10</th>\n",
       "      <th>s11</th>\n",
       "      <th>s12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exp_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020140</td>\n",
       "      <td>0.081164</td>\n",
       "      <td>0.046799</td>\n",
       "      <td>0.080794</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.055159</td>\n",
       "      <td>0.093193</td>\n",
       "      <td>0.058218</td>\n",
       "      <td>0.020610</td>\n",
       "      <td>0.071776</td>\n",
       "      <td>0.037899</td>\n",
       "      <td>0.066838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exp_1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.158808</td>\n",
       "      <td>0.037643</td>\n",
       "      <td>0.107039</td>\n",
       "      <td>0.262354</td>\n",
       "      <td>0.021680</td>\n",
       "      <td>0.112563</td>\n",
       "      <td>0.082086</td>\n",
       "      <td>0.083350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014638</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.123679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exp_1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.165128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161716</td>\n",
       "      <td>0.290950</td>\n",
       "      <td>0.068935</td>\n",
       "      <td>0.118747</td>\n",
       "      <td>0.094958</td>\n",
       "      <td>0.093735</td>\n",
       "      <td>0.024552</td>\n",
       "      <td>0.013541</td>\n",
       "      <td>0.028713</td>\n",
       "      <td>0.129184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exp_10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042981</td>\n",
       "      <td>0.087288</td>\n",
       "      <td>0.035596</td>\n",
       "      <td>0.092976</td>\n",
       "      <td>0.014878</td>\n",
       "      <td>0.094003</td>\n",
       "      <td>0.083272</td>\n",
       "      <td>0.084605</td>\n",
       "      <td>0.012392</td>\n",
       "      <td>0.059649</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>0.072118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exp_10</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.180660</td>\n",
       "      <td>0.021611</td>\n",
       "      <td>0.093696</td>\n",
       "      <td>0.285043</td>\n",
       "      <td>0.095564</td>\n",
       "      <td>0.130708</td>\n",
       "      <td>0.113519</td>\n",
       "      <td>0.127640</td>\n",
       "      <td>0.011734</td>\n",
       "      <td>0.024132</td>\n",
       "      <td>0.010197</td>\n",
       "      <td>0.140246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>exp_8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.081683</td>\n",
       "      <td>0.033165</td>\n",
       "      <td>0.105040</td>\n",
       "      <td>0.319787</td>\n",
       "      <td>0.105994</td>\n",
       "      <td>0.082605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108795</td>\n",
       "      <td>0.064975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036798</td>\n",
       "      <td>0.111022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>exp_8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.111730</td>\n",
       "      <td>0.015665</td>\n",
       "      <td>0.128014</td>\n",
       "      <td>0.331796</td>\n",
       "      <td>0.135531</td>\n",
       "      <td>0.058358</td>\n",
       "      <td>0.010763</td>\n",
       "      <td>0.088152</td>\n",
       "      <td>0.094879</td>\n",
       "      <td>0.013920</td>\n",
       "      <td>0.013347</td>\n",
       "      <td>0.161240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>exp_9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063205</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.088759</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.077716</td>\n",
       "      <td>0.004590</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>0.097105</td>\n",
       "      <td>0.087168</td>\n",
       "      <td>0.071016</td>\n",
       "      <td>0.095851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>exp_9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.174650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158850</td>\n",
       "      <td>0.097460</td>\n",
       "      <td>0.069699</td>\n",
       "      <td>0.115715</td>\n",
       "      <td>0.046565</td>\n",
       "      <td>0.061763</td>\n",
       "      <td>0.040231</td>\n",
       "      <td>0.027781</td>\n",
       "      <td>0.037776</td>\n",
       "      <td>0.195518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>exp_9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.132391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>0.278131</td>\n",
       "      <td>0.159930</td>\n",
       "      <td>0.092295</td>\n",
       "      <td>0.114139</td>\n",
       "      <td>0.078319</td>\n",
       "      <td>0.046285</td>\n",
       "      <td>0.023038</td>\n",
       "      <td>0.044428</td>\n",
       "      <td>0.150090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Treatments  Time        s1        s2        s3        s4        s5  \\\n",
       "0        exp_1   0.0  0.020140  0.081164  0.046799  0.080794  0.000743   \n",
       "1        exp_1   8.0  0.158808  0.037643  0.107039  0.262354  0.021680   \n",
       "2        exp_1  16.0  0.165128  0.000000  0.161716  0.290950  0.068935   \n",
       "3       exp_10   0.0  0.042981  0.087288  0.035596  0.092976  0.014878   \n",
       "4       exp_10   8.0  0.180660  0.021611  0.093696  0.285043  0.095564   \n",
       "..         ...   ...       ...       ...       ...       ...       ...   \n",
       "187      exp_8   8.0  0.081683  0.033165  0.105040  0.319787  0.105994   \n",
       "188      exp_8  16.0  0.111730  0.015665  0.128014  0.331796  0.135531   \n",
       "189      exp_9   0.0  0.063205  0.002620  0.088759  0.001612  0.012696   \n",
       "190      exp_9   8.0  0.174650  0.000000  0.158850  0.097460  0.069699   \n",
       "191      exp_9  16.0  0.132391  0.000000  0.159500  0.278131  0.159930   \n",
       "\n",
       "           s6        s7        s8        s9       s10       s11       s12  \n",
       "0    0.055159  0.093193  0.058218  0.020610  0.071776  0.037899  0.066838  \n",
       "1    0.112563  0.082086  0.083350  0.000000  0.014638  0.005025  0.123679  \n",
       "2    0.118747  0.094958  0.093735  0.024552  0.013541  0.028713  0.129184  \n",
       "3    0.094003  0.083272  0.084605  0.012392  0.059649  0.001639  0.072118  \n",
       "4    0.130708  0.113519  0.127640  0.011734  0.024132  0.010197  0.140246  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "187  0.082605  0.000000  0.108795  0.064975  0.000000  0.036798  0.111022  \n",
       "188  0.058358  0.010763  0.088152  0.094879  0.013920  0.013347  0.161240  \n",
       "189  0.077716  0.004590  0.071100  0.097105  0.087168  0.071016  0.095851  \n",
       "190  0.115715  0.046565  0.061763  0.040231  0.027781  0.037776  0.195518  \n",
       "191  0.092295  0.114139  0.078319  0.046285  0.023038  0.044428  0.150090  \n",
       "\n",
       "[192 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# used later for model validation\n",
    "# gLV_data = pd.read_csv(\"gLV_data/DSM_processed_mono.csv\")\n",
    "gLV_data = pd.read_csv(\"gLV_data/gLV_data_for_CR.csv\")\n",
    "gLV_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['s1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11',\n",
       "       's12'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get species names\n",
    "species = gLV_data.columns.values[2:]\n",
    "species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to make predictions on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimensions \n",
    "n_s = len(species)\n",
    "n_r = 3\n",
    "# input to NN includes species, resources (and maybe also time) \n",
    "n_x = n_s + n_r\n",
    "\n",
    "# CR parameters \n",
    "d = -3.*np.ones(n_s)\n",
    "C = np.random.uniform(-1., 0., [n_r, n_s])\n",
    "P = np.random.uniform(-5., -1., [n_r, n_s])\n",
    "K = np.ones(n_r)\n",
    "\n",
    "# dimension of hidden layer\n",
    "n_h = 4\n",
    "\n",
    "# map to hidden dimension\n",
    "p_std = 1./np.sqrt(n_x)\n",
    "W1 = p_std*np.random.randn(n_h, n_x)\n",
    "b1 = np.random.randn(n_h)\n",
    "\n",
    "# parameters to compute efficiency matrix\n",
    "p_std = 1./np.sqrt(n_h)\n",
    "W2 = p_std*np.random.randn(n_r+2*n_s, n_h) \n",
    "b2 = np.random.randn(n_r+2*n_s)\n",
    "\n",
    "# concatenate parameter initial guess\n",
    "params = np.concatenate((d, W1.flatten(), b1, C.flatten(), W2.flatten(), b2.flatten(), P.flatten(), K))\n",
    "\n",
    "# set prior so that C is sparse \n",
    "W1prior = np.zeros_like(W1)\n",
    "b1prior = np.zeros_like(b1)\n",
    "Cprior = -5.*np.ones([n_r, n_s]) \n",
    "Pprior = -5.*np.ones([n_r, n_s])\n",
    "W2prior = np.zeros_like(W2)\n",
    "b2prior = np.zeros_like(b2)\n",
    "\n",
    "# concatenate prior \n",
    "prior = np.concatenate((d, W1prior.flatten(), b1prior, Cprior.flatten(), W2prior.flatten(), b2prior.flatten(), Pprior.flatten(), K))\n",
    "\n",
    "n_params = len(params)\n",
    "n_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using consumer resource model  \n",
    "def system(t, x, params): \n",
    "    \n",
    "    # species \n",
    "    s = x[:n_s]\n",
    "    \n",
    "    # resources\n",
    "    r = jnp.exp(x[n_s:])\n",
    "    \n",
    "    # compute state \n",
    "    state = jnp.concatenate((s, r))\n",
    "    \n",
    "    # death rate\n",
    "    d = jnp.exp(params[:n_s])\n",
    "    \n",
    "    # map to hidden layer\n",
    "    W1 = np.reshape(params[n_s:n_s+n_x*n_h], [n_h, n_x])\n",
    "    b1 = params[n_s+n_x*n_h:n_s+n_x*n_h+n_h]\n",
    "    h1 = jnp.tanh(W1@state + b1)\n",
    "    \n",
    "    # maximum consumption rate parameters\n",
    "    Cmax = jnp.exp(np.reshape(params[n_s+n_x*n_h+n_h:n_s+n_x*n_h+n_h+n_r*n_s], [n_r, n_s]))\n",
    "    \n",
    "    # attractiveness of resource i to species j / consumption efficiency\n",
    "    W2 = np.reshape(params[n_s+n_x*n_h+n_h+n_r*n_s:n_s+n_x*n_h+n_h+n_r*n_s+(n_r+2*n_s)*n_h], [n_r+2*n_s, n_h])\n",
    "    b2 = np.reshape(params[n_s+n_x*n_h+n_h+n_r*n_s+(n_r+2*n_s)*n_h:n_s+n_x*n_h+n_h+n_r*n_s+(n_r+2*n_s)*n_h+n_r+2*n_s], [n_r+2*n_s])\n",
    "    h2 = jax.nn.sigmoid(W2@h1 + b2)\n",
    "    \n",
    "    # divide hidden layer into resource availability, species growth efficiency, resource production efficiency\n",
    "    f = h2[:n_r]\n",
    "    g = h2[n_r:n_r+n_s]\n",
    "    h = h2[n_r+n_s:]\n",
    "    \n",
    "    # update Consumption matrix according to resource attractiveness \n",
    "    C = jnp.einsum(\"i,ij->ij\", f, Cmax)\n",
    "    \n",
    "    # max production rate\n",
    "    Pmax = jnp.exp(jnp.reshape(params[n_s+n_x*n_h+n_h+n_r*n_s+(n_r+2*n_s)*n_h+n_r+2*n_s:n_s+n_x*n_h+n_h+n_r*n_s+(n_r+2*n_s)*n_h+n_r+2*n_s+n_r*n_s], [n_r, n_s]))\n",
    "    K = jnp.exp(params[n_s+n_x*n_h+n_h+n_r*n_s+(n_r+2*n_s)*n_h+n_r+2*n_s+n_r*n_s:])\n",
    "    \n",
    "    # scaled production rate\n",
    "    P = jnp.einsum(\"ij,j->ij\", Pmax, h)\n",
    "    \n",
    "    # rate of change of species \n",
    "    dsdt = s*(g*(C.T@r) - d)\n",
    "\n",
    "    # rate of change of log of resources \n",
    "    dlrdt = (1. - r/K) * ((P-C)@s) \n",
    "\n",
    "    return jnp.append(dsdt, dlrdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define observation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define observation matrices \n",
    "O = np.zeros([n_s, n_s+n_r])\n",
    "O[:n_s,:n_s] = np.eye(n_s)\n",
    "O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model to mono culture data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.88281056 -0.42854083 -2.92016665]\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 1.00e-05\n",
      "Total weighted fitting error: 2.854\n",
      "Total weighted fitting error: 2.415\n",
      "Total weighted fitting error: 2.137\n",
      "Total weighted fitting error: 1.868\n",
      "Total weighted fitting error: 1.796\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 1.795741331635591\n",
      "       x: [-1.779e+00  6.008e-02 ...  8.453e-01  9.997e-01]\n",
      "     nit: 5\n",
      "     jac: [-8.832e-02 -1.574e+00 ...  3.313e-01  2.203e-04]\n",
      "    nfev: 11\n",
      "    njev: 11\n",
      "    nhev: 5\n",
      "Evidence -279.563\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 1.95e-06\n",
      "Total weighted fitting error: 17.326\n",
      "Total weighted fitting error: 16.420\n",
      "Total weighted fitting error: 16.349\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 16.348757386031778\n",
      "       x: [-1.795e+00  4.605e-03 ...  8.557e-01  9.997e-01]\n",
      "     nit: 3\n",
      "     jac: [-3.696e-02 -2.663e+00 ...  5.295e-01  2.794e-04]\n",
      "    nfev: 4\n",
      "    njev: 4\n",
      "    nhev: 3\n",
      "Evidence 1838.928\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 4.57e-07\n",
      "Total weighted fitting error: 131.796\n",
      "Total weighted fitting error: 128.410\n",
      "Total weighted fitting error: 126.230\n",
      "Total weighted fitting error: 124.627\n",
      "Total weighted fitting error: 124.026\n",
      "Total weighted fitting error: 121.367\n",
      "Total weighted fitting error: 120.714\n",
      "Total weighted fitting error: 114.636\n",
      "Total weighted fitting error: 110.896\n",
      "Total weighted fitting error: 107.158\n",
      "Total weighted fitting error: 104.780\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 104.77960682352537\n",
      "       x: [-8.119e-01  3.178e-01 ...  1.080e+00  9.975e-01]\n",
      "     nit: 11\n",
      "     jac: [-7.918e+00  3.575e+00 ... -3.083e-01 -1.555e-02]\n",
      "    nfev: 16\n",
      "    njev: 16\n",
      "    nhev: 11\n",
      "Evidence 3372.622\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 2.31e-07\n",
      "Total weighted fitting error: 386.897\n",
      "Total weighted fitting error: 376.291\n",
      "Total weighted fitting error: 374.823\n",
      "Total weighted fitting error: 362.146\n",
      "Total weighted fitting error: 361.532\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 361.53246689567396\n",
      "       x: [-6.509e-01  4.272e-01 ...  1.202e+00  9.978e-01]\n",
      "     nit: 5\n",
      "     jac: [-1.309e+01 -1.087e+00 ...  2.295e+00 -8.984e-04]\n",
      "    nfev: 7\n",
      "    njev: 7\n",
      "    nhev: 5\n",
      "Evidence 4241.304\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 1.91e-07\n",
      "Total weighted fitting error: 538.593\n",
      "Total weighted fitting error: 525.683\n",
      "Total weighted fitting error: 524.220\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 524.2201527062533\n",
      "       x: [-5.251e-01  4.778e-01 ...  1.245e+00  9.981e-01]\n",
      "     nit: 3\n",
      "     jac: [-3.243e+01  4.749e+01 ... -1.995e+00  8.490e-04]\n",
      "    nfev: 4\n",
      "    njev: 4\n",
      "    nhev: 3\n",
      "Evidence 4597.808\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 2.29e-07\n",
      "Total weighted fitting error: 614.830\n",
      "Total weighted fitting error: 602.641\n",
      "Total weighted fitting error: 586.132\n",
      "Total weighted fitting error: 581.765\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 581.7645682270465\n",
      "       x: [-5.413e-01  6.076e-01 ...  1.350e+00  9.984e-01]\n",
      "     nit: 4\n",
      "     jac: [ 1.861e+01  2.045e+01 ... -4.166e+00  9.719e-02]\n",
      "    nfev: 5\n",
      "    njev: 5\n",
      "    nhev: 4\n",
      "Evidence 4777.508\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 3.49e-07\n",
      "Total weighted fitting error: 636.718\n",
      "Total weighted fitting error: 629.611\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 629.6112904197878\n",
      "       x: [-3.839e-01  6.747e-01 ...  1.386e+00  9.988e-01]\n",
      "     nit: 2\n",
      "     jac: [-8.317e+01  1.426e+02 ... -1.428e+01 -5.230e-02]\n",
      "    nfev: 3\n",
      "    njev: 3\n",
      "    nhev: 2\n",
      "Evidence 4906.603\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 5.41e-07\n",
      "Total weighted fitting error: 677.877\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 677.8765636146876\n",
      "       x: [-3.844e-01  6.773e-01 ...  1.386e+00  9.988e-01]\n",
      "     nit: 1\n",
      "     jac: [ 3.163e+01 -1.752e+02 ...  3.137e+01  1.390e-02]\n",
      "    nfev: 2\n",
      "    njev: 2\n",
      "    nhev: 1\n",
      "Evidence 4982.042\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 7.44e-07\n",
      "Total weighted fitting error: 700.147\n",
      "Total weighted fitting error: 691.390\n",
      "Total weighted fitting error: 676.682\n",
      "Total weighted fitting error: 675.334\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 675.3336217696186\n",
      "       x: [-4.360e-01  7.700e-01 ...  1.369e+00  9.992e-01]\n",
      "     nit: 4\n",
      "     jac: [ 1.508e+01 -9.087e-01 ...  1.123e+01 -1.347e-02]\n",
      "    nfev: 5\n",
      "    njev: 5\n",
      "    nhev: 4\n",
      "Evidence 5035.064\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 1.04e-06\n",
      "Total weighted fitting error: 708.674\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 708.6738011955973\n",
      "       x: [-4.356e-01  7.705e-01 ...  1.369e+00  9.992e-01]\n",
      "     nit: 1\n",
      "     jac: [-2.172e+01 -6.277e+01 ...  2.222e+01  3.049e-02]\n",
      "    nfev: 2\n",
      "    njev: 2\n",
      "    nhev: 1\n",
      "Evidence 5067.808\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 1.36e-06\n",
      "Total weighted fitting error: 709.075\n",
      "Total weighted fitting error: 708.033\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 708.0325085201662\n",
      "       x: [-4.140e-01  8.172e-01 ...  1.352e+00  9.993e-01]\n",
      "     nit: 2\n",
      "     jac: [-2.985e+01 -4.999e+01 ...  1.956e+01 -7.195e-03]\n",
      "    nfev: 4\n",
      "    njev: 4\n",
      "    nhev: 2\n",
      "Evidence 5095.500\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 1.69e-06\n",
      "Total weighted fitting error: 727.909\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 727.9091431766296\n",
      "       x: [-4.137e-01  8.181e-01 ...  1.352e+00  9.993e-01]\n",
      "     nit: 1\n",
      "     jac: [-2.587e+01 -1.550e+02 ...  3.245e+01  2.100e-02]\n",
      "    nfev: 2\n",
      "    njev: 2\n",
      "    nhev: 1\n",
      "Evidence 5110.230\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 2.06e-06\n",
      "Total weighted fitting error: 736.658\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 736.6580888505346\n",
      "       x: [-4.133e-01  8.187e-01 ...  1.352e+00  9.993e-01]\n",
      "     nit: 1\n",
      "     jac: [-2.320e+01 -9.262e+01 ...  2.927e+01  7.014e-03]\n",
      "    nfev: 2\n",
      "    njev: 2\n",
      "    nhev: 1\n",
      "Evidence 5118.286\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 2.44e-06\n",
      "Total weighted fitting error: 722.226\n",
      "Total weighted fitting error: 720.370\n",
      "Total weighted fitting error: 704.342\n",
      "Total weighted fitting error: 704.135\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 704.1352104986613\n",
      "       x: [-3.346e-01  9.348e-01 ...  1.262e+00  9.996e-01]\n",
      "     nit: 4\n",
      "     jac: [-2.338e+01  1.617e+02 ... -6.219e+01 -2.909e-02]\n",
      "    nfev: 7\n",
      "    njev: 7\n",
      "    nhev: 4\n",
      "Evidence 5156.313\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 2.73e-06\n",
      "Total weighted fitting error: 734.426\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 734.4261406166546\n",
      "       x: [-3.346e-01  9.358e-01 ...  1.261e+00  9.996e-01]\n",
      "     nit: 1\n",
      "     jac: [-5.901e+00 -3.846e+02 ...  8.971e+01  3.914e-02]\n",
      "    nfev: 2\n",
      "    njev: 2\n",
      "    nhev: 1\n",
      "Evidence 5172.583\n",
      "Updating precision...\n",
      "Total samples: 1536, Updated regularization: 3.08e-06\n",
      "Total weighted fitting error: 743.814\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 743.8144144052758\n",
      "       x: [-3.346e-01  9.366e-01 ...  1.261e+00  9.996e-01]\n",
      "     nit: 1\n",
      "     jac: [-1.030e+01 -2.609e+02 ...  8.430e+01  2.366e-02]\n",
      "    nfev: 2\n",
      "    njev: 2\n",
      "    nhev: 1\n",
      "Evidence 5177.237\n",
      "Pass count  1\n",
      "Elapsed time 121.89s\n"
     ]
    }
   ],
   "source": [
    "r0 = np.random.uniform(-3, 0, n_r)\n",
    "print(r0)\n",
    "\n",
    "model = ODE(system = system, \n",
    "            dataframe=gLV_data,\n",
    "            C=O,\n",
    "            CRparams = params, \n",
    "            r0 = r0,\n",
    "            prior = prior,\n",
    "            species = species,\n",
    "            alpha_0=1e-5,\n",
    "            verbose=True)\n",
    "\n",
    "# fit to data \n",
    "t0 = time.time()\n",
    "model.fit(evidence_tol=1e-3, nlp_tol=1e-3, patience=1, max_fails=1)\n",
    "print(\"Elapsed time {:.2f}s\".format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(self):\n",
    "    # loop over each sample in dataset\n",
    "    for n_t, (t_eval, Y_batch) in self.dataset.items():\n",
    "\n",
    "        # split samples into batches\n",
    "        n_samples = Y_batch.shape[0]\n",
    "        for batch_inds in np.array_split(np.arange(n_samples), n_samples//self.batch_size):\n",
    "            \n",
    "            # run model using current parameters, output = [n_time, self.n_sys_vars]\n",
    "            outputs = np.nan_to_num(self.batchODEZ(t_eval, Y_batch[batch_inds], self.params[:self.n_r], self.params[self.n_r:]))\n",
    "            \n",
    "            \n",
    "def forward(self):\n",
    "    # loop over each sample in dataset\n",
    "    for n_t, (t_eval, Y_batch) in self.dataset.items():\n",
    "\n",
    "        # split samples into batches\n",
    "        for Y_measured in Y_batch:\n",
    "\n",
    "            # run model using current parameters, output = [n_time, self.n_sys_vars]\n",
    "            output = np.nan_to_num(self.runODEZ(t_eval, Y_measured, self.params[:self.n_r], self.params[self.n_r:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.batch_size = 4\n",
    "model.batch_size\n",
    "\n",
    "model.batchODEZ = jit(vmap(model.runODEZ, (None, 0, None, None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983 ms Â± 3.79 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit batch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765 ms Â± 11.8 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit forward(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
